{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "\n",
        "A Decision Tree is a supervised learning algorithm used for both classification and regression, but it is most commonly applied in classification problems.\n",
        "\n",
        "It looks like a tree structure, where:\n",
        "\n",
        "Root Node -the first feature used to split the data.\n",
        "\n",
        "Internal Nodes-decision points based on features.\n",
        "\n",
        "Branches-possible outcomes of a decision.\n",
        "\n",
        "Leaf Nodes-the final class label (prediction).\n",
        "\n",
        "How it Works in Classification\n",
        "\n",
        "i)Start at the root node- The algorithm selects the feature that best separates the classes (using metrics like Gini Index, Entropy, or Information Gain).\n",
        "ii)Split the dataset-Data is divided into subsets based on the feature’s values.\n",
        "iii)Repeat recursively-Each subset is further split using the same process.\n",
        "iv)Stop condition-The splitting stops when:\n",
        "All data in a node belongs to the same class,\n",
        "No more features are left, or\n",
        "A maximum depth/limit is reached.\n",
        "v)Prediction-A new observation is classified by following the decisions from the root to a leaf node."
      ],
      "metadata": {
        "id": "0k066HzNQq3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Gini measures how often a randomly chosen sample would be incorrectly classified if it was randomly labeled according to the distribution of classes in a node.\n",
        "\n",
        "Entropy measures the amount of uncertainty or disorder in a node.\n",
        "\n",
        "How They Impact Splits\n",
        "\n",
        "A Decision Tree algorithm (like CART or ID3) uses these impurity measures to decide the best feature to split on:\n",
        "\n",
        "For each possible split, it calculates the impurity (Gini or Entropy) of the child nodes.\n",
        "\n",
        "It measures how much impurity decreases (this is called Information Gain when using Entropy).\n",
        "\n",
        "The split that gives the largest reduction in impurity is chosen.\n"
      ],
      "metadata": {
        "id": "J4tcTE_1UcBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 3- What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Pre-Pruning (Early Stopping)\n",
        "\n",
        "Definition: Stop the tree from growing too deep during training.\n",
        "\n",
        "How: Apply constraints like:\n",
        "\n",
        "-Maximum depth\n",
        "\n",
        "-Minimum samples per leaf/node\n",
        "\n",
        "-Minimum impurity decrease\n",
        "\n",
        "Goal: Prevent overfitting before it happens.\n",
        "\n",
        "✅ Practical Advantage:\n",
        "Faster training and simpler trees → useful when working with large datasets.\n",
        "\n",
        "Post-Pruning (Pruning after Full Growth)\n",
        "\n",
        "Definition: First, grow the tree fully, then prune (cut back) branches that don’t add much predictive power.\n",
        "\n",
        "How:\n",
        "\n",
        "-Cost-complexity pruning (CART)\n",
        "\n",
        "-Reduced-error pruning (C4.5)\n",
        "\n",
        "Goal: Simplify the model while keeping accuracy.\n",
        "\n",
        "✅ Practical Advantage:\n",
        "Often leads to better generalization, because the pruning decisions are based on the entire dataset.\n"
      ],
      "metadata": {
        "id": "UmScHYPbWcBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "nformation Gain (IG)\n",
        "\n",
        "Definition: Information Gain measures the reduction in impurity (uncertainty) after a dataset is split on a feature.\n",
        "\n",
        "It tells us how much “information” a feature gives us about the target variable.\n",
        "\n",
        "Why It’s Important\n",
        "\n",
        "-Decision Trees choose splits that maximize Information Gain.\n",
        "\n",
        "-Higher Information Gain = the split makes the child nodes purer (more homogeneous).\n",
        "\n",
        "-Without IG (or similar impurity measures), the tree would split randomly and not learn useful patterns.\n"
      ],
      "metadata": {
        "id": "QiCzF4raW9Wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Real-World Applications of Decision Trees\n",
        "\n",
        "i)Healthcare-Diagnosing diseases based on patient symptoms/test results.\n",
        "\n",
        "ii)Finance-Credit scoring, loan approval (predicting risk of default).\n",
        "\n",
        "iii)Marketing-Customer segmentation, predicting churn, targeting ads.\n",
        "\n",
        "iv)Retail/E-commerce-Product recommendation, demand forecasting.\n",
        "\n",
        "v)Manufacturing-Quality control, defect detection.\n",
        "\n",
        "vi)Education-Predicting student performance based on study habits, attendance, etc.\n",
        "\n",
        "Advantages of Decision Trees\n",
        "\n",
        "i)Easy to understand & interpret → Works like a flowchart, no complex math needed.\n",
        "\n",
        "ii)Handles categorical & numerical data → Flexible with different data types.\n",
        "\n",
        "iii)No feature scaling required → No need for normalization/standardization.\n",
        "\n",
        "iv)Captures nonlinear relationships → Works well when the relationship between features and target is complex.\n",
        "\n",
        "v)Can handle missing values (depending on implementation).\n",
        "\n",
        "Limitations of Decision Trees\n",
        "\n",
        "i)Overfitting-Trees can become too deep and memorize training data.\n",
        "\n",
        "ii)Unstable-Small changes in data can lead to very different trees.\n",
        "\n",
        "iii)Biased towards features with more levels-May favor categorical variables with many categories.\n",
        "\n",
        "iv)Less accurate compared to ensemble methods-Alone, a single tree is weaker than Random Forest or Gradient Boosted Trees."
      ],
      "metadata": {
        "id": "PuUaeo0iXbwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "WcQaSndYZyOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrk5oW_6Z_7Q",
        "outputId": "8487115b-ed91-4008-8280-e00defb82494"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree."
      ],
      "metadata": {
        "id": "B-Zm-RqpaJ81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train fully-grown Decision Tree (no depth limit)\n",
        "full_tree = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "limited_tree = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "y_pred_limited = limited_tree.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of fully-grown tree: \", accuracy_full)\n",
        "print(\"Accuracy of tree with max_depth=3: \", accuracy_limited)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbQSpxUFaRTt",
        "outputId": "d55f0e1e-e8b8-45bf-a2f6-e1273029e831"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of fully-grown tree:  1.0\n",
            "Accuracy of tree with max_depth=3:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n"
      ],
      "metadata": {
        "id": "7s5kLzuSaXcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data        # Features\n",
        "y = housing.target      # Target values (median house value)\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(criterion=\"squared_error\", random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "JrrK7LCjaecZ",
        "outputId": "5364eae9-edcf-4dfb-8bac-639dd30b8f92"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 26) (ipython-input-3383668803.py, line 26)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3383668803.py\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    print(\"Mean Squared Error\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "_5aA8F8Jaj2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],          # tree depth options\n",
        "    'min_samples_split': [2, 4, 6, 8, 10]     # minimum samples to split a node\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "\n",
        "# Perform GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate model with best parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with best parameters:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkJRDpI-bEx-",
        "outputId": "1f65cb38-942b-4a23-f82d-1b7e7b356880"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy with best parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "\n",
        "Step 1: Handling Missing Values\n",
        "\n",
        "Identify missing values in the dataset using isnull() or info().\n",
        "\n",
        "Decide on a strategy depending on the data type:\n",
        "\n",
        "Numerical features → fill with mean, median, or use advanced imputation (like KNN Imputer).\n",
        "\n",
        "Categorical features → fill with mode or “Unknown” category.\n",
        "\n",
        "Optionally, remove rows/columns with too many missing values if imputation is not meaningful.\n",
        "\n",
        "Example:\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "Step 2: Encoding Categorical Features\n",
        "\n",
        "Identify categorical features.\n",
        "\n",
        "Use encoding techniques:\n",
        "\n",
        "Label Encoding → for ordinal categories.\n",
        "\n",
        "One-Hot Encoding → for nominal categories with no order.\n",
        "\n",
        "Step 3: Train a Decision Tree Model\n",
        "\n",
        "Split the dataset into train/test sets.\n",
        "\n",
        "Initialize a Decision Tree Classifier:\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "Make predictions:\n",
        "\n",
        "y_pred = dt.predict(X_test)\n",
        "\n",
        "Step 4: Tune Hyperparameters\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV to find the best parameters.\n",
        "\n",
        "Key parameters to tune:\n",
        "\n",
        "max_depth → controls tree depth to prevent overfitting\n",
        "\n",
        "min_samples_split - minimum samples needed to split a node\n",
        "\n",
        "min_samples_leaf - minimum samples at a leaf\n",
        "\n",
        "criterion - gini or entropy\n",
        "\n",
        "Step 5: Evaluate Performance\n",
        "\n",
        "Use metrics suited for classification:\n",
        "\n",
        "Accuracy - overall correctness\n",
        "\n",
        "Precision & Recall - especially important in healthcare (to reduce false negatives)\n",
        "\n",
        "F1-score - balance between precision & recall\n",
        "\n",
        "ROC-AUC - performance across thresholds\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "Step 6: Business Value\n",
        "\n",
        "Early Detection - Identify high-risk patients before symptoms worsen.\n",
        "\n",
        "Resource Optimization - Prioritize medical tests and interventions efficiently.\n",
        "\n",
        "Personalized Care - Tailor treatment plans for patients based on risk prediction.\n",
        "\n",
        "Cost Reduction - Avoid unnecessary procedures and hospitalizations.\n",
        "\n"
      ],
      "metadata": {
        "id": "oU9GgNBabHYi"
      }
    }
  ]
}